---
title: 'Medical Question Answering using Simple Transformers T5 Model'
date: 1-April-2022
permalink: /posts/2022/02/2022_transformer_deid//
tags:
  - Transformers
  - Deep Learning
  - Medical Question Answering
  - T5 Transformer
---

This is a fun post. We find a medical textbook online: 1000 medical questions answered (Kumar and Clark). Using Linux pdf2text we convert the PDF to a plain text file. Next, we use R to parse the text file into a structured array of 1) questions and 2) answers. We fine-tune the simple transformer T5 encoder/decoder model on these medical question answer dataset. We feed the model test sentences (never seen before by the model), and assess the types of answers generated by the model.  

The Linux/R scripts used to pre-process the data are available: [here](../files/2022_04_parse_book_pdf2txt.sh) and [here](../files/2022_04_Parse_KumarClarkBook_To_QuestionAnswerPair_Dataset.R).

A Jupyter notebook illustrating how to fine-tune the T5 encoder/decoder transformers model is given [here](../files/2022_04_SimpleTransformers_T5_QA_MedQuestionAnswer_June2021.ipynb).

The model does not really work particularly well, in the sense that the sentences returned in response to the complex medical questions are somewhat non-sensical. Nonetheless, it is interesting to observe what the model generates (i.e. linguistic structure is reasonable, but medical knowledge is obviously lacking). Going forward it would be interesting to investigate the impact of: 1) a larger training corpora (i.e. >1000 questions/answers), and 2) more exhaustive hyper-parameter tuning. 

