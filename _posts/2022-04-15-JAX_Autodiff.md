---
title: 'JAX Automatic Differentiation for Introductory Statistics and Probability'
date: 2022-04-15
permalink: /posts/2022/04/jax_autodiff//
tags:
  - JAX
  - Automatic Differentiation
  - Statistics
  - Probability
---

In this post I link to a GitHub repository I created which demonstrates how one can apply Automatic Differentiation (using Python JAX modules) to illustrate core statistics and probability principles.

The repository contains several Jupyter Notebook files which exemplify how JAX can be used for:
- Linear Algebra
- Random Number Generation
- Statistics and Maximum Likelihood Estimation
- Automatic Differentiation for Linear Regression and other Generalized Linear Models

JAX can powerfully be integrated with hardware accelerators (e.g. GPU/TPU devices), to greatly reduce computational time required to perform core statistical/probabilistic inference.

As new examples illustrates the flexibility/utility of JAX (and Automatic Differentiation) in statistics/probability I will add them to the repository.

A link to the GitHub repository is given [here](https://github.com/meaneych/JAX_Examples).

A link to the official JAX website is provided [here](https://github.com/google/jax). The official JAX example/tutorial pages contain many helpful notebooks illustrating the power of JAX in automatic differentiation, neural programming, and beyond... 

