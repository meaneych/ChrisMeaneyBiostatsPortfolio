{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_7w7lBnijmo"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "##  Transformers token classification pipeline/fine-tuning for NER\n",
        "##  From tutorial: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
        "##\n",
        "## Modified by Author: Chris Meaney\n",
        "## Date: June 2021\n",
        "##\n",
        "## Purpose: apply transformers NER module over i2b2 2014 DEID dataset (train/val results; with hyper-parm tuning; final eval - best model - on test)\n",
        "##\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUxfVGRE5_5w",
        "outputId": "ed616420-3ac1-430b-9687-dfdb02e4a41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 24 15:11:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "## Print information about the specific NVIDIA GPU which COLAB has assigned to this session\n",
        "!nvidia-smi "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf6z99smijmt",
        "outputId": "51c19028-9f8e-4ae4-9a84-8335d4d4a6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sinfo\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "Collecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sinfo\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7899 sha256=c2e79dd2a9adef7ecbabae682c47f690ea7fe99262968e487b19271b59a13c5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ca/56/344d532fe53e855ccd6549795d370588ab8123907eecf4cf30\n",
            "Successfully built sinfo\n",
            "Installing collected packages: stdlib-list, sinfo\n",
            "Successfully installed sinfo-0.3.4 stdlib-list-0.8.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 13.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 36.3 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.3 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=49648b2b609258932e9c5b47fb66a5312548c0b4988f8e679ba9b64bee8c9ec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "##########################\n",
        "## Dependency modules\n",
        "##########################\n",
        "\n",
        "## For system info\n",
        "!pip install sinfo\n",
        "from sinfo import sinfo\n",
        "\n",
        "## For os tasks\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "## For timing\n",
        "import time\n",
        "\n",
        "## Pandas for data wrangling (import data)\n",
        "import pandas as pd\n",
        "\n",
        "## Used to display pandas data frame in a nice HTML format\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "## Numpy for numerics\n",
        "import random\n",
        "import numpy as np\n",
        "## Do I set seed for reproducibility? - How will this work on PyTorch, Transformers, etc. (i.e. is there a gloabl seed; or is this np.seed sufficient)\n",
        "np.random.seed(12345)\n",
        "\n",
        "## For pickling numpy arrays\n",
        "import pickle as pkl\n",
        "\n",
        "## sklearn for eval metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "## sklearn model selection tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Torch (for base NN layers/act-funs, loss, train/updates, etc.)\n",
        "!pip install torch \n",
        "import torch\n",
        "\n",
        "## Transformers \n",
        "! pip install transformers \n",
        "import transformers\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "## Datasets for CONLL example\n",
        "! pip install datasets \n",
        "from datasets import load_dataset, load_metric\n",
        "from datasets import ClassLabel, Sequence\n",
        "from datasets import Dataset\n",
        "\n",
        "## For sequence evaluation functions (to run against CONLL-NER format datasets)\n",
        "## https://pypi.org/project/seqeval/0.0.10/\n",
        "! pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkzkksDwijmu"
      },
      "outputs": [],
      "source": [
        "## Options for printing more rows/columns in Jupyter Notebook\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVbBiCMFtvu3",
        "outputId": "aeeb1ee6-67d6-46e4-d77c-87140c864f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "################################################\n",
        "## Connect to Google Colab\n",
        "################################################\n",
        "\n",
        "\n",
        "## Read in data from Google Drive account (this will force mount step, authentication step, etc.)\n",
        "## https://stackoverflow.com/questions/48340341/how-to-read-csv-to-dataframe-in-google-colab\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4JjrjFspVil"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "## Specific transformer model/architecture\n",
        "##########################\n",
        "\n",
        "# model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"bert-large-uncased\"\n",
        "# model_checkpoint = \"albert-base-v2\"\n",
        "# model_checkpoint = \"albert-xxlarge-v2\"\n",
        "# model_checkpoint = \"distilbert-base-uncased\"\n",
        "# model_checkpoint = \"xlm-roberta-base\"\n",
        "# model_checkpoint = \"xlm-roberta-large\"\n",
        "\n",
        "## Warning: for roberta models; need to instantiate tokenizer with add_prefix_space=True\n",
        "\n",
        "# model_checkpoint = \"roberta-base\"\n",
        "# model_checkpoint = \"distilroberta-base\"\n",
        "model_checkpoint = \"roberta-large\"\n",
        "\n",
        "#################\n",
        "## Batch size\n",
        "#################\n",
        "batch_size = 1\n",
        "\n",
        "############################\n",
        "## Number training epochs\n",
        "############################\n",
        "n_train_epochs = 5\n",
        "\n",
        "###################\n",
        "## Learning Rate\n",
        "###################\n",
        "learn_rate = 2e-5 \n",
        "\n",
        "###########################\n",
        "## Weight decay (L2 regularization - on final weight layer? or all layers?)\n",
        "###########################\n",
        "wt_decay = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V6LQ9wJ0_XH"
      },
      "outputs": [],
      "source": [
        "## Model prefix string - will be used as prefix for models results\n",
        "model_prefix = 'model=' + model_checkpoint + \"_numepochs=\" + str(n_train_epochs) + \"_learnrate=\" + str(learn_rate) + '_wtdecay=' + str(wt_decay)\n",
        "# model_prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5j5MAA2z1xn"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "## Paths to model/output dir\n",
        "##########################################################\n",
        "model_path = \"gdrive/My Drive/Colab Notebooks/transformer_model_dir/\" + model_checkpoint + \"/\"\n",
        "\n",
        "output_path = \"gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/\" + model_prefix + \"/\"\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    shutil.rmtree(output_path)\n",
        "\n",
        "os.makedirs(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tULFHvfJfj_n"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "## Use pandas to import data, and store as data.frame\n",
        "##########################################################\n",
        "dat = pd.read_csv('gdrive/My Drive/ColabData/bio_df_st.csv', encoding='latin1')\n",
        "# dat.head(n=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jum-gYwunfW"
      },
      "outputs": [],
      "source": [
        "## Check is tok_text is \"string\"; if True then keep; if False (since int/float/None/etc.) then delete\n",
        "\n",
        "# dat['tok_text_flag'] = dat.tok_text.isnull() \n",
        "# dat.tok_text_flag.value_counts()\n",
        "\n",
        "# dat['tok_text_flag'] = dat.tok_text.str.isnumeric() \n",
        "# dat.tok_text_flag.value_counts()\n",
        "\n",
        "dat['tok_text_flag'] = dat.tok_text.isnull() | dat.tok_text.str.isnumeric() \n",
        "# dat.tok_text_flag.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_FE56Ql8jpb"
      },
      "outputs": [],
      "source": [
        "## Drop these above rows from the data.frame\n",
        "dat = dat[dat['tok_text_flag']==False]\n",
        "# dat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN06Cg7b3l3x"
      },
      "outputs": [],
      "source": [
        "## Map the bio tags to integer indices\n",
        "codes, unique = pd.factorize(dat['bio'])\n",
        "dat['bio_int'] = codes\n",
        "# dat.bio_int.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hln39mUDgiRB",
        "outputId": "280803cb-35b7-4ea2-c794-89d55969cd43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "## Group the rows of the dataframe by doc_id\n",
        "dat_group = dat.groupby(['doc_id'],as_index=False)['is_test', 'bio', 'bio_r', 'bio_int', 'tok_text'].agg(lambda x: list(x))\n",
        "\n",
        "## Print head of data\n",
        "# dat_group.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNvUdL_Mfqzm"
      },
      "outputs": [],
      "source": [
        "## Create flag for train/test datasets\n",
        "dat_group['is_test_flag'] = [is_test[0] for is_test in dat_group.is_test]\n",
        "# dat_group.is_test_flag.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWNTDJ4BfrBg"
      },
      "outputs": [],
      "source": [
        "## Create train and test datasets\n",
        "train_dat = dat_group.loc[dat_group['is_test_flag'] == False, ['doc_id','bio','bio_int','tok_text']]\n",
        "test_dat = dat_group.loc[dat_group['is_test_flag'] == True, ['doc_id','bio','bio_int','tok_text']]\n",
        "\n",
        "# [train_dat.shape, test_dat.shape]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCo33Vhqimja"
      },
      "outputs": [],
      "source": [
        "## Further sample the training dataset into two distinct chunks (i.e. train and val)\n",
        "train_size = 500\n",
        "test_size = train_dat.shape[0] - train_size\n",
        "\n",
        "train_dat, val_dat = train_test_split(train_dat, train_size=train_size, test_size=test_size)\n",
        "\n",
        "# [train_dat.shape, val_dat.shape, test_dat.shape]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGCEU1B3ffDg"
      },
      "outputs": [],
      "source": [
        "## Inspect what one of the datasets above looks like\n",
        "# train_dat.head(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUd8Sqpjhs4v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm7-ymN1hs73"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl2-Q9XcimnV"
      },
      "outputs": [],
      "source": [
        "## Get list/set of unique tags\n",
        "# dat.bio.value_counts()\n",
        "#dat.bio.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVHF-cDrledn"
      },
      "outputs": [],
      "source": [
        "## Get names of IDs \n",
        "label_list = dat.bio.unique().tolist()\n",
        "# label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZbZOURzlehH"
      },
      "outputs": [],
      "source": [
        "## Get number of unique BIO tags for the i2b2 DEID NER task\n",
        "num_tags = len(label_list)\n",
        "# num_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl0SNosZHvS0"
      },
      "outputs": [],
      "source": [
        "#pd.DataFrame(pd.crosstab(dat.bio,dat.bio_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAJZ2GV8-v_G"
      },
      "outputs": [],
      "source": [
        "## Create cross tab dataFrame\n",
        "bio_ct = pd.DataFrame(pd.crosstab(dat.bio,dat.bio_int))\n",
        "\n",
        "## Get ijx representation of matrix\n",
        "bio_ct_vals = bio_ct.transpose().values.flatten()\n",
        "bio_ct_j = bio_ct.columns.values.repeat(bio_ct.shape[0])\n",
        "bio_ct_i = bio_ct.index.to_list()*bio_ct.shape[1]\n",
        "# [len(bio_ct_i), len(bio_ct_j), len(bio_ct_vals)]\n",
        "\n",
        "## Create COO format array\n",
        "bio_ct_long = pd.DataFrame({'i': bio_ct_i,\n",
        "                            'j': bio_ct_j,\n",
        "                            'x': bio_ct_vals})\n",
        "\n",
        "#bio_ct_long = bio_ct_long[['x']!=0]\n",
        "bio_ct_long = bio_ct_long[bio_ct_long['x']!=0]\n",
        "# bio_ct_long\n",
        "\n",
        "## Write the map to csv for use later if needed\n",
        "bio_ct_long_fpath = output_path + \"bio_ct_long.csv\"\n",
        "\n",
        "bio_ct_long.to_csv(path_or_buf=bio_ct_long_fpath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3R6HZzqCX5Q"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqQNLeLQ-wBg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoAqrIZ7imqA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvVTPNuoqBbQ"
      },
      "outputs": [],
      "source": [
        "###################################################\n",
        "## Install the tokenizer (note it will be specific to the model define above)\n",
        "###################################################   \n",
        "\n",
        "if model_checkpoint=='roberta-base':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
        "elif model_checkpoint=='roberta-large':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_prefix_space=True)\n",
        "elif model_checkpoint=='distilroberta-base':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYjSnzaQqBeF"
      },
      "outputs": [],
      "source": [
        "## Assertion/check against the particular tokenizer installed\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u0xA-gPrSI2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI8MuijauVIR"
      },
      "outputs": [],
      "source": [
        "##################################################################\n",
        "## Function to apply transormers tokenizer to sequence; then re-align labels to match newly encoded (new-length) sequence\n",
        "##################################################################\n",
        "label_all_tokens = True\n",
        "\n",
        "## Note: if any of the token elements are 'None' or str.isnumeric=True then I think this will fail?\n",
        "## Note: I handled this above by deleting these problematic tokens. That said, I could have handled by assigning to new string?? 's' + 'old_token'\n",
        "def tokenize_and_align_labels(tokens, tags):\n",
        "    tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tllheSJxqaCJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMpIDVXbqaIV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkJKo6PQdnj_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHEj-_zTDOu5"
      },
      "outputs": [],
      "source": [
        "##################################################################################\n",
        "## Batch encode tokens/attention-mask/labels for train val and test datasets\n",
        "##################################################################################\n",
        "\n",
        "## Training data\n",
        "train_encode = tokenize_and_align_labels(tokens=train_dat.tok_text.to_list(), tags=train_dat.bio_int.to_list())\n",
        "\n",
        "## Validation data\n",
        "val_encode = tokenize_and_align_labels(tokens=val_dat.tok_text.to_list(), tags=val_dat.bio_int.to_list())\n",
        "\n",
        "## Test data\n",
        "test_encode = tokenize_and_align_labels(tokens=test_dat.tok_text.to_list(), tags=test_dat.bio_int.to_list())\n",
        "\n",
        "## Check attributes/shape of train/val/test encoded datasets\n",
        "# [[len(train_encode['input_ids']), len(val_encode['input_ids']), len(test_encode['input_ids'])],\n",
        "# [len(train_encode['attention_mask']), len(val_encode['attention_mask']), len(test_encode['attention_mask'])],\n",
        "# [len(train_encode['labels']), len(val_encode['labels']), len(test_encode['labels'])]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH8q5ZtTv_IC"
      },
      "outputs": [],
      "source": [
        "## Create dataframe with columns \n",
        "train_df = pd.DataFrame({'input_ids':train_encode['input_ids'],\n",
        "                         'attention_mask':train_encode['attention_mask'],\n",
        "                         'labels':train_encode['labels'],})\n",
        "\n",
        "val_df = pd.DataFrame({'input_ids':val_encode['input_ids'],\n",
        "                         'attention_mask':val_encode['attention_mask'],\n",
        "                         'labels':val_encode['labels'],})\n",
        "\n",
        "test_df = pd.DataFrame({'input_ids':test_encode['input_ids'],\n",
        "                         'attention_mask':test_encode['attention_mask'],\n",
        "                         'labels':test_encode['labels'],})\n",
        "\n",
        "#train_df.head(n=5)\n",
        "#val_df.head(n=5)\n",
        "#test_df.head(n=5)\n",
        "\n",
        "## Convert each of the above objects into a HuggingFace Dataset (note: based on Apache Arrow dataset)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I0QUZ-hDPMT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NouBS_Lr8iP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgc3AY7jus0D"
      },
      "outputs": [],
      "source": [
        "###############################################################\n",
        "## Instantiate a transformers Token Classification of NER model\n",
        "##\n",
        "## Model type should be one of BigBirdConfig, ConvBertConfig, LayoutLMConfig, DistilBertConfig, CamembertConfig, FlaubertConfig, XLMConfig, XLMRobertaConfig, LongformerConfig, RobertaConfig, SqueezeBertConfig, BertConfig, MegatronBertConfig, MobileBertConfig, XLNetConfig, AlbertConfig, ElectraConfig, FunnelConfig, MPNetConfig, DebertaConfig, DebertaV2Config, IBertConfig.\n",
        "###############################################################\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_path, \n",
        "                                                        num_labels=num_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEndTLwx-GRz"
      },
      "outputs": [],
      "source": [
        "# help(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIqktG2QOdP",
        "outputId": "e4d63c4b-22bd-484f-c44d-af35874ffaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaForTokenClassification(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (18): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (19): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (20): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (21): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (22): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (23): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=1024, out_features=41, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVjVcMitZQLm",
        "outputId": "0d305a5e-c868-4fa9-a7d8-451cd038580d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354352169"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "## Count total number of model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmuVRafkZQN7",
        "outputId": "e624b300-85bd-4999-969a-df743a61e1a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354352169"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "## Count total number of \"trainable\" model parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXTtljCTZQR4",
        "outputId": "d479566e-eac9-46be-ccc0-f2ab5c947f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------+------------+\n",
            "|                          Modules                           | Parameters |\n",
            "+------------------------------------------------------------+------------+\n",
            "|         roberta.embeddings.word_embeddings.weight          |  51471360  |\n",
            "|       roberta.embeddings.position_embeddings.weight        |   526336   |\n",
            "|      roberta.embeddings.token_type_embeddings.weight       |    1024    |\n",
            "|            roberta.embeddings.LayerNorm.weight             |    1024    |\n",
            "|             roberta.embeddings.LayerNorm.bias              |    1024    |\n",
            "|    roberta.encoder.layer.0.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.0.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.0.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.0.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.0.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.0.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.0.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.0.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.0.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.0.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.0.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.0.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.0.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.0.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.0.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.0.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.1.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.1.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.1.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.1.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.1.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.1.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.1.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.1.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.1.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.1.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.1.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.1.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.1.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.1.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.1.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.1.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.2.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.2.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.2.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.2.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.2.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.2.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.2.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.2.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.2.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.2.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.2.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.2.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.2.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.2.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.2.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.2.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.3.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.3.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.3.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.3.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.3.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.3.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.3.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.3.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.3.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.3.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.3.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.3.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.3.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.3.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.3.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.3.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.4.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.4.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.4.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.4.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.4.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.4.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.4.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.4.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.4.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.4.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.4.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.4.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.4.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.4.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.4.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.4.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.5.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.5.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.5.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.5.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.5.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.5.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.5.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.5.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.5.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.5.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.5.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.5.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.5.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.5.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.5.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.5.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.6.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.6.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.6.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.6.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.6.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.6.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.6.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.6.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.6.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.6.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.6.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.6.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.6.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.6.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.6.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.6.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.7.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.7.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.7.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.7.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.7.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.7.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.7.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.7.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.7.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.7.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.7.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.7.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.7.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.7.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.7.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.7.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.8.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.8.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.8.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.8.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.8.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.8.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.8.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.8.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.8.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.8.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.8.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.8.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.8.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.8.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.8.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.8.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.9.attention.self.query.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.9.attention.self.query.bias      |    1024    |\n",
            "|     roberta.encoder.layer.9.attention.self.key.weight      |  1048576   |\n",
            "|      roberta.encoder.layer.9.attention.self.key.bias       |    1024    |\n",
            "|    roberta.encoder.layer.9.attention.self.value.weight     |  1048576   |\n",
            "|     roberta.encoder.layer.9.attention.self.value.bias      |    1024    |\n",
            "|   roberta.encoder.layer.9.attention.output.dense.weight    |  1048576   |\n",
            "|    roberta.encoder.layer.9.attention.output.dense.bias     |    1024    |\n",
            "| roberta.encoder.layer.9.attention.output.LayerNorm.weight  |    1024    |\n",
            "|  roberta.encoder.layer.9.attention.output.LayerNorm.bias   |    1024    |\n",
            "|     roberta.encoder.layer.9.intermediate.dense.weight      |  4194304   |\n",
            "|      roberta.encoder.layer.9.intermediate.dense.bias       |    4096    |\n",
            "|        roberta.encoder.layer.9.output.dense.weight         |  4194304   |\n",
            "|         roberta.encoder.layer.9.output.dense.bias          |    1024    |\n",
            "|      roberta.encoder.layer.9.output.LayerNorm.weight       |    1024    |\n",
            "|       roberta.encoder.layer.9.output.LayerNorm.bias        |    1024    |\n",
            "|    roberta.encoder.layer.10.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.10.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.10.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.10.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.10.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.10.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.10.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.10.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.10.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.10.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.10.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.10.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.10.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.10.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.10.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.10.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.11.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.11.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.11.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.11.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.11.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.11.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.11.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.11.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.11.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.11.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.11.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.11.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.11.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.11.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.11.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.11.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.12.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.12.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.12.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.12.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.12.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.12.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.12.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.12.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.12.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.12.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.12.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.12.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.12.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.12.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.12.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.12.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.13.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.13.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.13.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.13.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.13.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.13.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.13.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.13.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.13.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.13.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.13.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.13.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.13.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.13.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.13.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.13.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.14.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.14.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.14.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.14.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.14.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.14.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.14.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.14.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.14.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.14.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.14.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.14.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.14.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.14.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.14.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.14.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.15.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.15.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.15.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.15.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.15.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.15.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.15.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.15.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.15.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.15.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.15.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.15.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.15.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.15.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.15.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.15.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.16.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.16.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.16.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.16.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.16.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.16.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.16.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.16.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.16.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.16.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.16.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.16.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.16.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.16.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.16.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.16.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.17.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.17.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.17.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.17.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.17.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.17.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.17.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.17.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.17.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.17.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.17.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.17.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.17.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.17.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.17.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.17.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.18.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.18.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.18.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.18.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.18.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.18.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.18.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.18.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.18.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.18.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.18.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.18.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.18.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.18.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.18.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.18.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.19.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.19.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.19.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.19.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.19.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.19.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.19.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.19.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.19.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.19.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.19.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.19.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.19.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.19.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.19.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.19.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.20.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.20.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.20.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.20.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.20.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.20.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.20.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.20.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.20.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.20.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.20.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.20.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.20.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.20.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.20.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.20.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.21.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.21.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.21.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.21.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.21.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.21.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.21.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.21.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.21.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.21.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.21.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.21.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.21.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.21.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.21.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.21.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.22.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.22.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.22.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.22.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.22.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.22.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.22.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.22.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.22.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.22.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.22.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.22.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.22.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.22.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.22.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.22.output.LayerNorm.bias       |    1024    |\n",
            "|    roberta.encoder.layer.23.attention.self.query.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.23.attention.self.query.bias     |    1024    |\n",
            "|     roberta.encoder.layer.23.attention.self.key.weight     |  1048576   |\n",
            "|      roberta.encoder.layer.23.attention.self.key.bias      |    1024    |\n",
            "|    roberta.encoder.layer.23.attention.self.value.weight    |  1048576   |\n",
            "|     roberta.encoder.layer.23.attention.self.value.bias     |    1024    |\n",
            "|   roberta.encoder.layer.23.attention.output.dense.weight   |  1048576   |\n",
            "|    roberta.encoder.layer.23.attention.output.dense.bias    |    1024    |\n",
            "| roberta.encoder.layer.23.attention.output.LayerNorm.weight |    1024    |\n",
            "|  roberta.encoder.layer.23.attention.output.LayerNorm.bias  |    1024    |\n",
            "|     roberta.encoder.layer.23.intermediate.dense.weight     |  4194304   |\n",
            "|      roberta.encoder.layer.23.intermediate.dense.bias      |    4096    |\n",
            "|        roberta.encoder.layer.23.output.dense.weight        |  4194304   |\n",
            "|         roberta.encoder.layer.23.output.dense.bias         |    1024    |\n",
            "|      roberta.encoder.layer.23.output.LayerNorm.weight      |    1024    |\n",
            "|       roberta.encoder.layer.23.output.LayerNorm.bias       |    1024    |\n",
            "|                     classifier.weight                      |   41984    |\n",
            "|                      classifier.bias                       |     41     |\n",
            "+------------------------------------------------------------+------------+\n",
            "Total Trainable Params: 354352169\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354352169"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "## Print table of number of parameters for each layer/module\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87IkQ5akus24"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "## Hyper-parameters from NER model\n",
        "##############################################################\n",
        "args = TrainingArguments(\n",
        "    output_dir='i2b2_output',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=learn_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=n_train_epochs,\n",
        "    weight_decay=wt_decay\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-LM2Gasus5P"
      },
      "outputs": [],
      "source": [
        "## Data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c58982d26ddf4df4828f0b1a23e24aae",
            "5ef58f824d0e439dae33469d75ceeeca",
            "d5a89819db944ab38db2b9aa743c55de",
            "fb9519b26f604793b54fedf5f690402e",
            "bed41815973640a7bc24e65103a402ef",
            "ed2e3c538384418eacd8c3c4db8ec63b",
            "17fda8b469db40a4895b9676d5d01c5c",
            "a88b7f28161040f1a4daf9b612ea1bba",
            "621cbc4f3cd24248b9d518a62eeac120",
            "0eb53e2443ae48d39b8d844a0fb2fea1",
            "c7318fdaf0074f8e9592c2bcdf171f17"
          ]
        },
        "id": "bYiocHnUus8B",
        "outputId": "ca0dc620-d64f-4432-aa1e-338f04a5936d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c58982d26ddf4df4828f0b1a23e24aae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Sequence evaluation metric (from CONLL - used to eval NER, etc. type tasks)\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8MRUlm2utB8"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "## Function to compute evaluation metrics on train/val/test samples\n",
        "###############################################\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpslTGMUutDv"
      },
      "outputs": [],
      "source": [
        "####################################################################\n",
        "## Specify a training function; this will train/fine-tune NER model; and print metrics on train/val sets\n",
        "####################################################################\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UiVA2WkkvarU",
        "outputId": "ed82456e-331e-4d08-9064-ec779fad10f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 43:08, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.108400</td>\n",
              "      <td>0.033407</td>\n",
              "      <td>0.910498</td>\n",
              "      <td>0.922327</td>\n",
              "      <td>0.916374</td>\n",
              "      <td>0.992366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.023900</td>\n",
              "      <td>0.025169</td>\n",
              "      <td>0.953558</td>\n",
              "      <td>0.948171</td>\n",
              "      <td>0.950857</td>\n",
              "      <td>0.995118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.013900</td>\n",
              "      <td>0.021850</td>\n",
              "      <td>0.957387</td>\n",
              "      <td>0.958198</td>\n",
              "      <td>0.957792</td>\n",
              "      <td>0.995885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.008600</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.957390</td>\n",
              "      <td>0.961446</td>\n",
              "      <td>0.959414</td>\n",
              "      <td>0.996058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.020149</td>\n",
              "      <td>0.965977</td>\n",
              "      <td>0.962293</td>\n",
              "      <td>0.964132</td>\n",
              "      <td>0.996472</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to i2b2_output/checkpoint-500\n",
            "Configuration saved in i2b2_output/checkpoint-500/config.json\n",
            "Model weights saved in i2b2_output/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in i2b2_output/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in i2b2_output/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to i2b2_output/checkpoint-1000\n",
            "Configuration saved in i2b2_output/checkpoint-1000/config.json\n",
            "Model weights saved in i2b2_output/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in i2b2_output/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in i2b2_output/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to i2b2_output/checkpoint-1500\n",
            "Configuration saved in i2b2_output/checkpoint-1500/config.json\n",
            "Model weights saved in i2b2_output/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in i2b2_output/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in i2b2_output/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to i2b2_output/checkpoint-2000\n",
            "Configuration saved in i2b2_output/checkpoint-2000/config.json\n",
            "Model weights saved in i2b2_output/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in i2b2_output/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in i2b2_output/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to i2b2_output/checkpoint-2500\n",
            "Configuration saved in i2b2_output/checkpoint-2500/config.json\n",
            "Model weights saved in i2b2_output/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in i2b2_output/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in i2b2_output/checkpoint-2500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##############################################################\n",
        "## Train the model - print per-epoch training/val metrics to console\n",
        "##############################################################\n",
        "t0 = time.time()\n",
        "trainer.train()\n",
        "t1 = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPmNLO35znxg"
      },
      "outputs": [],
      "source": [
        "## Print training time\n",
        "train_time = t1-t0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuk94E77H7lw"
      },
      "outputs": [],
      "source": [
        "#help(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "fn4dmA6FvauA",
        "outputId": "35cc3b83-e59c-432b-aec6-ecee96517cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [237/237 00:56]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "## Evaluate a trained model\n",
        "trainer_evalaute_metrics = trainer.evaluate()\n",
        "trainer_evaluate_metrics_df = pd.Series(trainer_evalaute_metrics)\n",
        "trainer_evaluate_metrics_df\n",
        "\n",
        "## Write train/val/test metrics dataFrame to disk\n",
        "trainer_df_fpath = output_path + \"trainer_df.csv\"\n",
        "\n",
        "trainer_evaluate_metrics_df.to_csv(path_or_buf=trainer_df_fpath, index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws8m-fbUds-4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ldAAbzvdtBT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "IovQbk0pfXQ-",
        "outputId": "c893e09c-b42a-4ec6-c2c8-8458b1512172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 500\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='737' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [237/237 03:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#############################################\n",
        "## Evaluate model on validation set - per tag analysis and overall analysis\n",
        "#############################################\n",
        "predictions_train, labels_train, _ = trainer.predict(train_dataset)\n",
        "predictions_train_max = np.argmax(predictions_train, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions_train = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions_train_max, labels_train)\n",
        "]\n",
        "true_labels_train = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions_train_max, labels_train)\n",
        "]\n",
        "\n",
        "## See results of trained model applied to eval/test set (evalued on a per-tag basis - this is like sklearn.metrics.classification_report)\n",
        "train_metrics = metric.compute(predictions=true_predictions_train, references=true_labels_train)\n",
        "train_metrics_df = pd.DataFrame(train_metrics).transpose()\n",
        "train_metrics_df['dataset'] = 'train'\n",
        "train_metrics_df['phi_type'] = train_metrics_df.index\n",
        "#train_metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgSLsUCBE4K7"
      },
      "outputs": [],
      "source": [
        "## Train input-ids decoded to tokens (using character level tokenizer)\n",
        "train_tokens = [tokenizer.convert_ids_to_tokens(ids)[1:-1] for ids in train_df.input_ids]\n",
        "\n",
        "## Flatten lists\n",
        "train_tokens_long = [item for sublist in train_tokens for item in sublist]\n",
        "true_predictions_train_long = [item for sublist in true_predictions_train for item in sublist]\n",
        "true_labels_train_long = [item for sublist in true_labels_train for item in sublist]\n",
        "#[len(train_tokens_long), len(true_predictions_train_long), len(true_labels_train_long)]\n",
        "\n",
        "## Get docs_ids\n",
        "len_docs = [len(tokenizer.convert_ids_to_tokens(ids)[1:-1]) for ids in train_df.input_ids]\n",
        "doc_ids = train_dat.doc_id.repeat(len_docs)\n",
        "# len(doc_ids)\n",
        "\n",
        "## Put flat lists into pandas dataFrame\n",
        "train_preds_out = pd.DataFrame({'doc_id': doc_ids,\n",
        "                                  'tokens': train_tokens_long,\n",
        "                                  'pred': true_predictions_train_long,\n",
        "                                  'true_label': true_labels_train_long,\n",
        "                                  })\n",
        "\n",
        "## Denote that these are training data\n",
        "train_preds_out['dataset'] = 'train'\n",
        "# train_preds_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLm83diZ6qcs"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "## Save the array of scores/logits over: docs (i=1..500), words (p=1..512), bio-tags (t=1..41)\n",
        "#########################################\n",
        "fpath_prediction_train_arr = output_path + \"predictions_train_arr.npy\"\n",
        "\n",
        "np.save(fpath_prediction_train_arr, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_tHmjzJ8RxH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moqjzMuR8lc3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "disbjDkbvawy",
        "outputId": "b579dbd3-8fca-4f1a-9866-8e344ef00ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 237\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='974' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [237/237 04:54]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#############################################\n",
        "## Evaluate model on validation set - per tag analysis and overall analysis\n",
        "#############################################\n",
        "predictions_val, labels_val, _ = trainer.predict(val_dataset)\n",
        "predictions_val_max = np.argmax(predictions_val, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions_val = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions_val_max, labels_val)\n",
        "]\n",
        "true_labels_val = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions_val_max, labels_val)\n",
        "]\n",
        "\n",
        "## See results of trained model applied to eval/test set (evalued on a per-tag basis - this is like sklearn.metrics.classification_report)\n",
        "val_metrics = metric.compute(predictions=true_predictions_val, references=true_labels_val)\n",
        "val_metrics_df = pd.DataFrame(val_metrics).transpose()\n",
        "val_metrics_df['dataset'] = 'val'\n",
        "val_metrics_df['phi_type'] = val_metrics_df.index\n",
        "#val_metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Eg0NG3rJx_d"
      },
      "outputs": [],
      "source": [
        "## Validation input-ids decoded to tokens (using character level tokenizer)\n",
        "val_tokens = [tokenizer.convert_ids_to_tokens(ids)[1:-1] for ids in val_df.input_ids]\n",
        "\n",
        "## Flatten lists\n",
        "val_tokens_long = [item for sublist in val_tokens for item in sublist]\n",
        "true_predictions_val_long = [item for sublist in true_predictions_val for item in sublist]\n",
        "true_labels_val_long = [item for sublist in true_labels_val for item in sublist]\n",
        "#[len(val_tokens_long), len(true_predictions_val_long), len(true_labels_val_long)]\n",
        "\n",
        "## Get docs_ids\n",
        "len_docs = [len(tokenizer.convert_ids_to_tokens(ids)[1:-1]) for ids in val_df.input_ids]\n",
        "doc_ids = val_dat.doc_id.repeat(len_docs)\n",
        "# len(doc_ids)\n",
        "\n",
        "## Put flat lists into pandas dataFrame\n",
        "val_preds_out = pd.DataFrame({'doc_id': doc_ids,\n",
        "                                  'tokens': val_tokens_long,\n",
        "                                  'pred': true_predictions_val_long,\n",
        "                                  'true_label': true_labels_val_long,\n",
        "                                  })\n",
        "\n",
        "## Denote that these are training data\n",
        "val_preds_out['dataset'] = 'val'\n",
        "## val_preds_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_605w_DuAKyf"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "## Save the array of scores/logits over: docs (i=1..500), words (p=1..512), bio-tags (t=1..41)\n",
        "#########################################\n",
        "fpath_prediction_val_arr = output_path + \"predictions_val_arr.npy\"\n",
        "\n",
        "np.save(fpath_prediction_val_arr, predictions_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzpSCbcPdxno"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdSXo0_pdxp_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "0cu9BSnuva-8",
        "outputId": "ec4b34c4-b2c6-498f-918a-269ed6025da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 486\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1460' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [237/237 07:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#############################################\n",
        "## Evaluate model on test set - per tag analysis and overall analysis\n",
        "#############################################\n",
        "predictions_test, labels_test, _ = trainer.predict(test_dataset)\n",
        "predictions_test_max = np.argmax(predictions_test, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions_test = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions_test_max, labels_test)\n",
        "]\n",
        "true_labels_test = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions_test_max, labels_test)\n",
        "]\n",
        "\n",
        "## See results of trained model applied to eval/test set (evalued on a per-tag basis - this is like sklearn.metrics.classification_report)\n",
        "test_metrics = metric.compute(predictions=true_predictions_test, references=true_labels_test)\n",
        "test_metrics_df = pd.DataFrame(test_metrics).transpose()\n",
        "test_metrics_df['dataset'] = 'test'\n",
        "test_metrics_df['phi_type'] = test_metrics_df.index\n",
        "#test_metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7BgsyHCKVXQ"
      },
      "outputs": [],
      "source": [
        "## Test input-ids decoded to tokens (using character level tokenizer)\n",
        "test_tokens = [tokenizer.convert_ids_to_tokens(ids)[1:-1] for ids in test_df.input_ids]\n",
        "\n",
        "## Flatten lists\n",
        "test_tokens_long = [item for sublist in test_tokens for item in sublist]\n",
        "true_predictions_test_long = [item for sublist in true_predictions_test for item in sublist]\n",
        "true_labels_test_long = [item for sublist in true_labels_test for item in sublist]\n",
        "#[len(test_tokens_long), len(true_predictions_test_long), len(true_labels_test_long)]\n",
        "\n",
        "## Get docs_ids\n",
        "len_docs = [len(tokenizer.convert_ids_to_tokens(ids)[1:-1]) for ids in test_df.input_ids]\n",
        "doc_ids = test_dat.doc_id.repeat(len_docs)\n",
        "# len(doc_ids)\n",
        "\n",
        "## Put flat lists into pandas dataFrame\n",
        "test_preds_out = pd.DataFrame({'doc_id': doc_ids,\n",
        "                                  'tokens': test_tokens_long,\n",
        "                                  'pred': true_predictions_test_long,\n",
        "                                  'true_label': true_labels_test_long,\n",
        "                                  })\n",
        "\n",
        "## Denote that these are training data\n",
        "test_preds_out['dataset'] = 'test'\n",
        "# test_preds_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H0OGB40zeUl"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "## Save the array of scores/logits over: docs (i=1..500), words (p=1..512), bio-tags (t=1..41)\n",
        "#########################################\n",
        "fpath_prediction_test_arr = output_path + \"predictions_test_arr.npy\"\n",
        "\n",
        "np.save(fpath_prediction_test_arr, predictions_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVuv4StJAW_i"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwmIFVna81Oe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFRijdZv81RK"
      },
      "outputs": [],
      "source": [
        "##############################################################################\n",
        "## Combine train/val/test evaluation metrics dataframe and export to disk\n",
        "##############################################################################\n",
        "metrics_df = pd.concat([train_metrics_df, val_metrics_df, test_metrics_df])\n",
        "\n",
        "metrics_df['model_name'] = model_checkpoint\n",
        "metrics_df['num_epochs'] = n_train_epochs\n",
        "metrics_df['learning_rate'] = learn_rate\n",
        "metrics_df['weight_decay'] = wt_decay\n",
        "\n",
        "## Write train/val/test metrics dataFrame to disk\n",
        "metrics_df_fpath = output_path + \"overall_train_val_test_metrics_df.csv\"\n",
        "\n",
        "metrics_df.to_csv(path_or_buf=metrics_df_fpath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nVqvfpTWY7Us",
        "outputId": "2adc8a85-6ce1-4ac7-9204-00ee6e69a984"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a2dd1b33-e0d4-444b-892c-7711467c60fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>number</th>\n",
              "      <th>dataset</th>\n",
              "      <th>phi_type</th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_epochs</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>weight_decay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.981818</td>\n",
              "      <td>0.990826</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>AGE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CITY</th>\n",
              "      <td>0.996743</td>\n",
              "      <td>0.993506</td>\n",
              "      <td>0.995122</td>\n",
              "      <td>308.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>CITY</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COUNTRY</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.942857</td>\n",
              "      <td>0.970588</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>COUNTRY</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATE</th>\n",
              "      <td>0.998712</td>\n",
              "      <td>0.997428</td>\n",
              "      <td>0.998069</td>\n",
              "      <td>8552.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>DATE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DOCTOR</th>\n",
              "      <td>0.954726</td>\n",
              "      <td>0.951495</td>\n",
              "      <td>0.953107</td>\n",
              "      <td>1773.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>DOCTOR</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EMAIL</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>EMAIL</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FAX</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>FAX</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HEALTHPLAN</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>HEALTHPLAN</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HOSPITAL</th>\n",
              "      <td>0.997710</td>\n",
              "      <td>0.996949</td>\n",
              "      <td>0.997329</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IDNUM</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>IDNUM</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOCATION-OTHER</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>LOCATION-OTHER</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEDICALRECORD</th>\n",
              "      <td>0.994030</td>\n",
              "      <td>0.998501</td>\n",
              "      <td>0.996260</td>\n",
              "      <td>667.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>MEDICALRECORD</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORGANIZATION</th>\n",
              "      <td>0.950820</td>\n",
              "      <td>0.865672</td>\n",
              "      <td>0.906250</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>ORGANIZATION</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PATIENT</th>\n",
              "      <td>0.997040</td>\n",
              "      <td>0.996450</td>\n",
              "      <td>0.996745</td>\n",
              "      <td>1690.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>PATIENT</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PHONE</th>\n",
              "      <td>0.985019</td>\n",
              "      <td>0.966912</td>\n",
              "      <td>0.975881</td>\n",
              "      <td>272.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>PHONE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PROFESSION</th>\n",
              "      <td>0.943182</td>\n",
              "      <td>0.821782</td>\n",
              "      <td>0.878307</td>\n",
              "      <td>101.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>PROFESSION</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STATE</th>\n",
              "      <td>0.986486</td>\n",
              "      <td>0.986486</td>\n",
              "      <td>0.986486</td>\n",
              "      <td>148.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>STATE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STREET</th>\n",
              "      <td>0.989691</td>\n",
              "      <td>0.979592</td>\n",
              "      <td>0.984615</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>STREET</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USERNAME</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>124.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>USERNAME</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZIP</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>train</td>\n",
              "      <td>ZIP</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_precision</th>\n",
              "      <td>0.992253</td>\n",
              "      <td>0.992253</td>\n",
              "      <td>0.992253</td>\n",
              "      <td>0.992253</td>\n",
              "      <td>train</td>\n",
              "      <td>overall_precision</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_recall</th>\n",
              "      <td>0.988841</td>\n",
              "      <td>0.988841</td>\n",
              "      <td>0.988841</td>\n",
              "      <td>0.988841</td>\n",
              "      <td>train</td>\n",
              "      <td>overall_recall</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_f1</th>\n",
              "      <td>0.990544</td>\n",
              "      <td>0.990544</td>\n",
              "      <td>0.990544</td>\n",
              "      <td>0.990544</td>\n",
              "      <td>train</td>\n",
              "      <td>overall_f1</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_accuracy</th>\n",
              "      <td>0.999210</td>\n",
              "      <td>0.999210</td>\n",
              "      <td>0.999210</td>\n",
              "      <td>0.999210</td>\n",
              "      <td>train</td>\n",
              "      <td>overall_accuracy</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>AGE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CITY</th>\n",
              "      <td>0.892086</td>\n",
              "      <td>0.867133</td>\n",
              "      <td>0.879433</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>CITY</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COUNTRY</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>COUNTRY</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATE</th>\n",
              "      <td>0.989072</td>\n",
              "      <td>0.989072</td>\n",
              "      <td>0.989072</td>\n",
              "      <td>3935.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>DATE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DOCTOR</th>\n",
              "      <td>0.911868</td>\n",
              "      <td>0.905484</td>\n",
              "      <td>0.908665</td>\n",
              "      <td>857.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>DOCTOR</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FAX</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>FAX</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HOSPITAL</th>\n",
              "      <td>0.926056</td>\n",
              "      <td>0.908463</td>\n",
              "      <td>0.917175</td>\n",
              "      <td>579.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IDNUM</th>\n",
              "      <td>0.810811</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>IDNUM</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOCATION-OTHER</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>LOCATION-OTHER</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEDICALRECORD</th>\n",
              "      <td>0.997222</td>\n",
              "      <td>0.983562</td>\n",
              "      <td>0.990345</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>MEDICALRECORD</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORGANIZATION</th>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.594595</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>ORGANIZATION</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PATIENT</th>\n",
              "      <td>0.980874</td>\n",
              "      <td>0.971583</td>\n",
              "      <td>0.976207</td>\n",
              "      <td>739.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>PATIENT</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PHONE</th>\n",
              "      <td>0.891429</td>\n",
              "      <td>0.987342</td>\n",
              "      <td>0.936937</td>\n",
              "      <td>158.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>PHONE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PROFESSION</th>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>PROFESSION</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STATE</th>\n",
              "      <td>0.945946</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.927152</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>STATE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STREET</th>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.924528</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>STREET</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USERNAME</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>val</td>\n",
              "      <td>USERNAME</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_precision</th>\n",
              "      <td>0.965977</td>\n",
              "      <td>0.965977</td>\n",
              "      <td>0.965977</td>\n",
              "      <td>0.965977</td>\n",
              "      <td>val</td>\n",
              "      <td>overall_precision</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_recall</th>\n",
              "      <td>0.962293</td>\n",
              "      <td>0.962293</td>\n",
              "      <td>0.962293</td>\n",
              "      <td>0.962293</td>\n",
              "      <td>val</td>\n",
              "      <td>overall_recall</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_f1</th>\n",
              "      <td>0.964132</td>\n",
              "      <td>0.964132</td>\n",
              "      <td>0.964132</td>\n",
              "      <td>0.964132</td>\n",
              "      <td>val</td>\n",
              "      <td>overall_f1</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_accuracy</th>\n",
              "      <td>0.996472</td>\n",
              "      <td>0.996472</td>\n",
              "      <td>0.996472</td>\n",
              "      <td>0.996472</td>\n",
              "      <td>val</td>\n",
              "      <td>overall_accuracy</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>AGE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CITY</th>\n",
              "      <td>0.832765</td>\n",
              "      <td>0.862191</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>283.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>CITY</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COUNTRY</th>\n",
              "      <td>0.895349</td>\n",
              "      <td>0.740385</td>\n",
              "      <td>0.810526</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>COUNTRY</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATE</th>\n",
              "      <td>0.994151</td>\n",
              "      <td>0.992337</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>8221.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>DATE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DEVICE</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>DEVICE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DOCTOR</th>\n",
              "      <td>0.901330</td>\n",
              "      <td>0.907873</td>\n",
              "      <td>0.904590</td>\n",
              "      <td>1791.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>DOCTOR</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FAX</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>FAX</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HOSPITAL</th>\n",
              "      <td>0.902110</td>\n",
              "      <td>0.907470</td>\n",
              "      <td>0.904782</td>\n",
              "      <td>1178.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IDNUM</th>\n",
              "      <td>0.977901</td>\n",
              "      <td>0.977901</td>\n",
              "      <td>0.977901</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>IDNUM</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOCATION-OTHER</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>LOCATION-OTHER</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEDICALRECORD</th>\n",
              "      <td>0.987382</td>\n",
              "      <td>0.992076</td>\n",
              "      <td>0.989723</td>\n",
              "      <td>631.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>MEDICALRECORD</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORGANIZATION</th>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.440678</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>ORGANIZATION</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PATIENT</th>\n",
              "      <td>0.973239</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.968742</td>\n",
              "      <td>1848.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>PATIENT</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PHONE</th>\n",
              "      <td>0.949875</td>\n",
              "      <td>0.994751</td>\n",
              "      <td>0.971795</td>\n",
              "      <td>381.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>PHONE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PROFESSION</th>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.705263</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>PROFESSION</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STATE</th>\n",
              "      <td>0.816993</td>\n",
              "      <td>0.862069</td>\n",
              "      <td>0.838926</td>\n",
              "      <td>145.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>STATE</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STREET</th>\n",
              "      <td>0.883495</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>105.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>STREET</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USERNAME</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.981818</td>\n",
              "      <td>0.990826</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>USERNAME</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZIP</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>test</td>\n",
              "      <td>ZIP</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_precision</th>\n",
              "      <td>0.963024</td>\n",
              "      <td>0.963024</td>\n",
              "      <td>0.963024</td>\n",
              "      <td>0.963024</td>\n",
              "      <td>test</td>\n",
              "      <td>overall_precision</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_recall</th>\n",
              "      <td>0.960669</td>\n",
              "      <td>0.960669</td>\n",
              "      <td>0.960669</td>\n",
              "      <td>0.960669</td>\n",
              "      <td>test</td>\n",
              "      <td>overall_recall</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_f1</th>\n",
              "      <td>0.961845</td>\n",
              "      <td>0.961845</td>\n",
              "      <td>0.961845</td>\n",
              "      <td>0.961845</td>\n",
              "      <td>test</td>\n",
              "      <td>overall_f1</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_accuracy</th>\n",
              "      <td>0.996357</td>\n",
              "      <td>0.996357</td>\n",
              "      <td>0.996357</td>\n",
              "      <td>0.996357</td>\n",
              "      <td>test</td>\n",
              "      <td>overall_accuracy</td>\n",
              "      <td>roberta-large</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2dd1b33-e0d4-444b-892c-7711467c60fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2dd1b33-e0d4-444b-892c-7711467c60fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2dd1b33-e0d4-444b-892c-7711467c60fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   precision    recall        f1       number dataset           phi_type  \\\n",
              "AGE                 1.000000  0.981818  0.990826    55.000000   train                AGE   \n",
              "CITY                0.996743  0.993506  0.995122   308.000000   train               CITY   \n",
              "COUNTRY             1.000000  0.942857  0.970588    35.000000   train            COUNTRY   \n",
              "DATE                0.998712  0.997428  0.998069  8552.000000   train               DATE   \n",
              "DOCTOR              0.954726  0.951495  0.953107  1773.000000   train             DOCTOR   \n",
              "EMAIL               1.000000  0.888889  0.941176     9.000000   train              EMAIL   \n",
              "FAX                 1.000000  1.000000  1.000000    28.000000   train                FAX   \n",
              "HEALTHPLAN          0.000000  0.000000  0.000000     1.000000   train         HEALTHPLAN   \n",
              "HOSPITAL            0.997710  0.996949  0.997329  1311.000000   train           HOSPITAL   \n",
              "IDNUM               1.000000  1.000000  1.000000   167.000000   train              IDNUM   \n",
              "LOCATION-OTHER      0.000000  0.000000  0.000000     2.000000   train     LOCATION-OTHER   \n",
              "MEDICALRECORD       0.994030  0.998501  0.996260   667.000000   train      MEDICALRECORD   \n",
              "ORGANIZATION        0.950820  0.865672  0.906250    67.000000   train       ORGANIZATION   \n",
              "PATIENT             0.997040  0.996450  0.996745  1690.000000   train            PATIENT   \n",
              "PHONE               0.985019  0.966912  0.975881   272.000000   train              PHONE   \n",
              "PROFESSION          0.943182  0.821782  0.878307   101.000000   train         PROFESSION   \n",
              "STATE               0.986486  0.986486  0.986486   148.000000   train              STATE   \n",
              "STREET              0.989691  0.979592  0.984615    98.000000   train             STREET   \n",
              "USERNAME            1.000000  1.000000  1.000000   124.000000   train           USERNAME   \n",
              "ZIP                 1.000000  0.200000  0.333333     5.000000   train                ZIP   \n",
              "overall_precision   0.992253  0.992253  0.992253     0.992253   train  overall_precision   \n",
              "overall_recall      0.988841  0.988841  0.988841     0.988841   train     overall_recall   \n",
              "overall_f1          0.990544  0.990544  0.990544     0.990544   train         overall_f1   \n",
              "overall_accuracy    0.999210  0.999210  0.999210     0.999210   train   overall_accuracy   \n",
              "AGE                 1.000000  0.900000  0.947368    10.000000     val                AGE   \n",
              "CITY                0.892086  0.867133  0.879433   143.000000     val               CITY   \n",
              "COUNTRY             0.900000  0.900000  0.900000    10.000000     val            COUNTRY   \n",
              "DATE                0.989072  0.989072  0.989072  3935.000000     val               DATE   \n",
              "DOCTOR              0.911868  0.905484  0.908665   857.000000     val             DOCTOR   \n",
              "FAX                 0.000000  0.000000  0.000000     7.000000     val                FAX   \n",
              "HOSPITAL            0.926056  0.908463  0.917175   579.000000     val           HOSPITAL   \n",
              "IDNUM               0.810811  1.000000  0.895522    30.000000     val              IDNUM   \n",
              "LOCATION-OTHER      0.000000  0.000000  0.000000     1.000000     val     LOCATION-OTHER   \n",
              "MEDICALRECORD       0.997222  0.983562  0.990345   365.000000     val      MEDICALRECORD   \n",
              "ORGANIZATION        0.647059  0.550000  0.594595    20.000000     val       ORGANIZATION   \n",
              "PATIENT             0.980874  0.971583  0.976207   739.000000     val            PATIENT   \n",
              "PHONE               0.891429  0.987342  0.936937   158.000000     val              PHONE   \n",
              "PROFESSION          0.818182  0.734694  0.774194    49.000000     val         PROFESSION   \n",
              "STATE               0.945946  0.909091  0.927152    77.000000     val              STATE   \n",
              "STREET              0.907407  0.942308  0.924528    52.000000     val             STREET   \n",
              "USERNAME            1.000000  1.000000  1.000000    49.000000     val           USERNAME   \n",
              "overall_precision   0.965977  0.965977  0.965977     0.965977     val  overall_precision   \n",
              "overall_recall      0.962293  0.962293  0.962293     0.962293     val     overall_recall   \n",
              "overall_f1          0.964132  0.964132  0.964132     0.964132     val         overall_f1   \n",
              "overall_accuracy    0.996472  0.996472  0.996472     0.996472     val   overall_accuracy   \n",
              "AGE                 0.864865  0.864865  0.864865    37.000000    test                AGE   \n",
              "CITY                0.832765  0.862191  0.847222   283.000000    test               CITY   \n",
              "COUNTRY             0.895349  0.740385  0.810526   104.000000    test            COUNTRY   \n",
              "DATE                0.994151  0.992337  0.993243  8221.000000    test               DATE   \n",
              "DEVICE              0.000000  0.000000  0.000000     4.000000    test             DEVICE   \n",
              "DOCTOR              0.901330  0.907873  0.904590  1791.000000    test             DOCTOR   \n",
              "FAX                 0.000000  0.000000  0.000000     0.000000    test                FAX   \n",
              "HOSPITAL            0.902110  0.907470  0.904782  1178.000000    test           HOSPITAL   \n",
              "IDNUM               0.977901  0.977901  0.977901   181.000000    test              IDNUM   \n",
              "LOCATION-OTHER      0.000000  0.000000  0.000000     4.000000    test     LOCATION-OTHER   \n",
              "MEDICALRECORD       0.987382  0.992076  0.989723   631.000000    test      MEDICALRECORD   \n",
              "ORGANIZATION        0.722222  0.440678  0.547368    59.000000    test       ORGANIZATION   \n",
              "PATIENT             0.973239  0.964286  0.968742  1848.000000    test            PATIENT   \n",
              "PHONE               0.949875  0.994751  0.971795   381.000000    test              PHONE   \n",
              "PROFESSION          0.788235  0.705263  0.744444    95.000000    test         PROFESSION   \n",
              "STATE               0.816993  0.862069  0.838926   145.000000    test              STATE   \n",
              "STREET              0.883495  0.866667  0.875000   105.000000    test             STREET   \n",
              "USERNAME            1.000000  0.981818  0.990826    55.000000    test           USERNAME   \n",
              "ZIP                 0.000000  0.000000  0.000000     6.000000    test                ZIP   \n",
              "overall_precision   0.963024  0.963024  0.963024     0.963024    test  overall_precision   \n",
              "overall_recall      0.960669  0.960669  0.960669     0.960669    test     overall_recall   \n",
              "overall_f1          0.961845  0.961845  0.961845     0.961845    test         overall_f1   \n",
              "overall_accuracy    0.996357  0.996357  0.996357     0.996357    test   overall_accuracy   \n",
              "\n",
              "                      model_name  num_epochs  learning_rate  weight_decay  \n",
              "AGE                roberta-large           5        0.00002          0.01  \n",
              "CITY               roberta-large           5        0.00002          0.01  \n",
              "COUNTRY            roberta-large           5        0.00002          0.01  \n",
              "DATE               roberta-large           5        0.00002          0.01  \n",
              "DOCTOR             roberta-large           5        0.00002          0.01  \n",
              "EMAIL              roberta-large           5        0.00002          0.01  \n",
              "FAX                roberta-large           5        0.00002          0.01  \n",
              "HEALTHPLAN         roberta-large           5        0.00002          0.01  \n",
              "HOSPITAL           roberta-large           5        0.00002          0.01  \n",
              "IDNUM              roberta-large           5        0.00002          0.01  \n",
              "LOCATION-OTHER     roberta-large           5        0.00002          0.01  \n",
              "MEDICALRECORD      roberta-large           5        0.00002          0.01  \n",
              "ORGANIZATION       roberta-large           5        0.00002          0.01  \n",
              "PATIENT            roberta-large           5        0.00002          0.01  \n",
              "PHONE              roberta-large           5        0.00002          0.01  \n",
              "PROFESSION         roberta-large           5        0.00002          0.01  \n",
              "STATE              roberta-large           5        0.00002          0.01  \n",
              "STREET             roberta-large           5        0.00002          0.01  \n",
              "USERNAME           roberta-large           5        0.00002          0.01  \n",
              "ZIP                roberta-large           5        0.00002          0.01  \n",
              "overall_precision  roberta-large           5        0.00002          0.01  \n",
              "overall_recall     roberta-large           5        0.00002          0.01  \n",
              "overall_f1         roberta-large           5        0.00002          0.01  \n",
              "overall_accuracy   roberta-large           5        0.00002          0.01  \n",
              "AGE                roberta-large           5        0.00002          0.01  \n",
              "CITY               roberta-large           5        0.00002          0.01  \n",
              "COUNTRY            roberta-large           5        0.00002          0.01  \n",
              "DATE               roberta-large           5        0.00002          0.01  \n",
              "DOCTOR             roberta-large           5        0.00002          0.01  \n",
              "FAX                roberta-large           5        0.00002          0.01  \n",
              "HOSPITAL           roberta-large           5        0.00002          0.01  \n",
              "IDNUM              roberta-large           5        0.00002          0.01  \n",
              "LOCATION-OTHER     roberta-large           5        0.00002          0.01  \n",
              "MEDICALRECORD      roberta-large           5        0.00002          0.01  \n",
              "ORGANIZATION       roberta-large           5        0.00002          0.01  \n",
              "PATIENT            roberta-large           5        0.00002          0.01  \n",
              "PHONE              roberta-large           5        0.00002          0.01  \n",
              "PROFESSION         roberta-large           5        0.00002          0.01  \n",
              "STATE              roberta-large           5        0.00002          0.01  \n",
              "STREET             roberta-large           5        0.00002          0.01  \n",
              "USERNAME           roberta-large           5        0.00002          0.01  \n",
              "overall_precision  roberta-large           5        0.00002          0.01  \n",
              "overall_recall     roberta-large           5        0.00002          0.01  \n",
              "overall_f1         roberta-large           5        0.00002          0.01  \n",
              "overall_accuracy   roberta-large           5        0.00002          0.01  \n",
              "AGE                roberta-large           5        0.00002          0.01  \n",
              "CITY               roberta-large           5        0.00002          0.01  \n",
              "COUNTRY            roberta-large           5        0.00002          0.01  \n",
              "DATE               roberta-large           5        0.00002          0.01  \n",
              "DEVICE             roberta-large           5        0.00002          0.01  \n",
              "DOCTOR             roberta-large           5        0.00002          0.01  \n",
              "FAX                roberta-large           5        0.00002          0.01  \n",
              "HOSPITAL           roberta-large           5        0.00002          0.01  \n",
              "IDNUM              roberta-large           5        0.00002          0.01  \n",
              "LOCATION-OTHER     roberta-large           5        0.00002          0.01  \n",
              "MEDICALRECORD      roberta-large           5        0.00002          0.01  \n",
              "ORGANIZATION       roberta-large           5        0.00002          0.01  \n",
              "PATIENT            roberta-large           5        0.00002          0.01  \n",
              "PHONE              roberta-large           5        0.00002          0.01  \n",
              "PROFESSION         roberta-large           5        0.00002          0.01  \n",
              "STATE              roberta-large           5        0.00002          0.01  \n",
              "STREET             roberta-large           5        0.00002          0.01  \n",
              "USERNAME           roberta-large           5        0.00002          0.01  \n",
              "ZIP                roberta-large           5        0.00002          0.01  \n",
              "overall_precision  roberta-large           5        0.00002          0.01  \n",
              "overall_recall     roberta-large           5        0.00002          0.01  \n",
              "overall_f1         roberta-large           5        0.00002          0.01  \n",
              "overall_accuracy   roberta-large           5        0.00002          0.01  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyEtkSgW81UG"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "## Combine train/val/test predictions dataframe and export to disk\n",
        "################################################################################\n",
        "preds_out_df = pd.concat([train_preds_out, val_preds_out, test_preds_out])\n",
        "\n",
        "preds_out_df['model_name'] = model_checkpoint\n",
        "preds_out_df['num_epochs'] = n_train_epochs\n",
        "preds_out_df['learning_rate'] = learn_rate\n",
        "preds_out_df['weight_decay'] = wt_decay\n",
        "\n",
        "## Write train/val/test metrics dataFrame to disk\n",
        "preds_df_fpath = output_path + \"overall_train_val_test_preds_df.csv\"\n",
        "\n",
        "preds_out_df.to_csv(path_or_buf=preds_df_fpath, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X8lGKMpNGKu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOwxUaS8PHs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV0BLFUbaY1P",
        "outputId": "2104147b-3e8e-4acc-d1db-480a396ee562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/config.json\n",
            "Model weights saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/pytorch_model.bin\n",
            "tokenizer config file saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/tokenizer_config.json\n",
            "Special tokens file saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/tokenizer_config.json',\n",
              " 'gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/special_tokens_map.json',\n",
              " 'gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/vocab.json',\n",
              " 'gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/merges.txt',\n",
              " 'gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/added_tokens.json',\n",
              " 'gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/roberta-large/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "############################\n",
        "## Save final model and tokenizer to disk\n",
        "############################\n",
        "model_out_path = output_path + model_checkpoint\n",
        "tokenizer_out_path = output_path + model_checkpoint \n",
        "\n",
        "model.save_pretrained(model_out_path)\n",
        "tokenizer.save_pretrained(tokenizer_out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIKMWAdxaY3w",
        "outputId": "ae4d33fe-e023-4a6f-d3ec-cdd71b77d562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/trainer_model\n",
            "Configuration saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/trainer_model/config.json\n",
            "Model weights saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/trainer_model/pytorch_model.bin\n",
            "tokenizer config file saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/trainer_model/tokenizer_config.json\n",
            "Special tokens file saved in gdrive/My Drive/Colab Notebooks/transformer_model_output_dir/model=roberta-large_numepochs=5_learnrate=2e-05_wtdecay=0.01/trainer_model/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "####################################\n",
        "## Save the final trainer file to disk\n",
        "####################################\n",
        "trainer_out_path = output_path + 'trainer_model'\n",
        "\n",
        "trainer.save_model(trainer_out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCcp24vYbQx7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5RaGGgCaZKs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9YsRqWeNHw0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mONn8AvSNIDt"
      },
      "outputs": [],
      "source": [
        "## See what bio_tags map to what integers (bio_int)\n",
        "#bio_ct = pd.DataFrame(pd.crosstab(dat.bio,dat.bio_int)).to_dict()\n",
        "#bio_ct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gp6MH5r-NBhV",
        "outputId": "17465eac-4f88-4f0c-ada5-6f74025ba4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 1\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1462' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [237/237 08:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3cb04413-757a-4e11-bbc8-fab8f7df6b32\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>true_labels</th>\n",
              "      <th>true_preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ĠDate</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>:</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ĠJune</td>\n",
              "      <td>B-DATE</td>\n",
              "      <td>B-DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ġ2020</td>\n",
              "      <td>I-DATE</td>\n",
              "      <td>B-DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>:</td>\n",
              "      <td>I-DATE</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ĠPatient</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Ġ-</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ġchrist</td>\n",
              "      <td>B-PATIENT</td>\n",
              "      <td>B-PATIENT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>opher</td>\n",
              "      <td>B-PATIENT</td>\n",
              "      <td>B-PATIENT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Ġme</td>\n",
              "      <td>I-PATIENT</td>\n",
              "      <td>I-PATIENT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>aney</td>\n",
              "      <td>I-PATIENT</td>\n",
              "      <td>I-PATIENT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Ġ-</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Ġa</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Ġbi</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ost</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>at</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>istic</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ian</td>\n",
              "      <td>B-PROFESSION</td>\n",
              "      <td>I-PROFESSION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Ġat</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ĠUT</td>\n",
              "      <td>B-ORGANIZATION</td>\n",
              "      <td>B-HOSPITAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Ġpresented</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Ġto</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ĠDr</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>ĠJ</td>\n",
              "      <td>B-DOCTOR</td>\n",
              "      <td>B-DOCTOR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>.</td>\n",
              "      <td>B-DOCTOR</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ĠSmith</td>\n",
              "      <td>I-DOCTOR</td>\n",
              "      <td>I-DOCTOR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Ġwith</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Ġback</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Ġpain</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Ġfrom</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Ġsed</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>entary</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Ġlifestyle</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Ġand</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>ĠR</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SI</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3cb04413-757a-4e11-bbc8-fab8f7df6b32')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3cb04413-757a-4e11-bbc8-fab8f7df6b32 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3cb04413-757a-4e11-bbc8-fab8f7df6b32');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        tokens     true_labels    true_preds\n",
              "0        ĠDate               O             O\n",
              "1            :               O             O\n",
              "2        ĠJune          B-DATE        B-DATE\n",
              "3        Ġ2020          I-DATE        B-DATE\n",
              "4            :          I-DATE             O\n",
              "5     ĠPatient               O             O\n",
              "6           Ġ-               O             O\n",
              "7      Ġchrist       B-PATIENT     B-PATIENT\n",
              "8        opher       B-PATIENT     B-PATIENT\n",
              "9          Ġme       I-PATIENT     I-PATIENT\n",
              "10        aney       I-PATIENT     I-PATIENT\n",
              "11          Ġ-               O             O\n",
              "12          Ġa               O             O\n",
              "13         Ġbi    B-PROFESSION  B-PROFESSION\n",
              "14         ost    B-PROFESSION  B-PROFESSION\n",
              "15          at    B-PROFESSION  B-PROFESSION\n",
              "16       istic    B-PROFESSION  B-PROFESSION\n",
              "17         ian    B-PROFESSION  I-PROFESSION\n",
              "18         Ġat               O             O\n",
              "19         ĠUT  B-ORGANIZATION    B-HOSPITAL\n",
              "20  Ġpresented               O             O\n",
              "21         Ġto               O             O\n",
              "22         ĠDr               O             O\n",
              "23           .               O             O\n",
              "24          ĠJ        B-DOCTOR      B-DOCTOR\n",
              "25           .        B-DOCTOR             O\n",
              "26      ĠSmith        I-DOCTOR      I-DOCTOR\n",
              "27       Ġwith               O             O\n",
              "28       Ġback               O             O\n",
              "29       Ġpain               O             O\n",
              "30       Ġfrom               O             O\n",
              "31        Ġsed               O             O\n",
              "32      entary               O             O\n",
              "33  Ġlifestyle               O             O\n",
              "34        Ġand               O             O\n",
              "35          ĠR               O             O\n",
              "36          SI               O             O\n",
              "37           .               O             O"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "###########################################\n",
        "## Encode a random string and apply model.predict() method to see if it captures PHI needed to be DEID\n",
        "###########################################\n",
        "my_string = \"Date: June 2020: Patient - christopher meaney - a biostatistician at UT presented to Dr. J. Smith with back pain from sedentary lifestyle and RSI.\"\n",
        "\n",
        "## Tokenize string\n",
        "my_tokens = my_string.split(' ')\n",
        "# my_tokens\n",
        "\n",
        "## Get associated tags (labels for string)\n",
        "my_tags = ['O','B-DATE','I-DATE','O','O',\"B-PATIENT\",\"I-PATIENT\",'O','O','B-PROFESSSION','O','B-ORGANIZATION','O','O','O','B-DOCTOR',\"I-DOCTOR\",'O','O','O','O','O','O','O','O']\n",
        "my_tags_int = [0,1,12,0,0,11,10,0,0,6,0,5,0,0,0,2,4,0,0,0,0,0,0,0,0]\n",
        "\n",
        "## Check that token/tag length are the same\n",
        "# [len(my_tokens), len(my_tags), len(my_tags_int)]\n",
        "my_string_df = pd.DataFrame({'tokens': [my_tokens],\n",
        "              'bio': [my_tags],\n",
        "               'bio_int': [my_tags_int]})\n",
        "\n",
        "my_string_df_long = pd.DataFrame({'tokens': my_tokens,\n",
        "              'bio': my_tags,\n",
        "               'bio_int': my_tags_int})\n",
        "\n",
        "# my_string_df\n",
        "\n",
        "\n",
        "## Pass the dataframe to tokenizer\n",
        "my_string_encode = tokenize_and_align_labels(tokens=my_string_df.tokens.to_list(), tags=my_string_df.bio_int.to_list())\n",
        "\n",
        "## Convert tokenized input into pandas dataframe\n",
        "my_string_encoded_df = pd.DataFrame({'input_ids': my_string_encode['input_ids'],\n",
        "                              'attention_mask': my_string_encode['attention_mask'],\n",
        "                              'labels': my_string_encode['labels']})\n",
        "# my_string_encoded_df\n",
        "\n",
        "\n",
        "## Convert pandas dataFrame into HuggingFace Dataset (an Apache Arrow dataset)\n",
        "my_string_dataset = Dataset.from_pandas(my_string_encoded_df)\n",
        "#my_string_dataset[0]\n",
        "\n",
        "## Feed formatted string (in Dataset structure); to fine-tuned Transformer model (and obtain predictions)\n",
        "predictions, labels, _ = trainer.predict(my_string_dataset)\n",
        "predictions_max = np.argmax(predictions, axis=2)\n",
        "\n",
        "#predictions_max\n",
        "#labels.shape\n",
        "#type(predictions_max)\n",
        "\n",
        "\n",
        "## Feed formatted string (in Dataset structure); to fine-tuned Transformer model (and obtain predictions)\n",
        "predictions, labels, _ = trainer.predict(my_string_dataset)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "## Put into dataframe\n",
        "pd.DataFrame({'tokens': my_string_encode.tokens()[1:-1],\n",
        "              'true_labels': true_labels[0],\n",
        "              'true_preds': true_predictions[0]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3h8Mrc1NBkC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkaHXKctNBq2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLK9zjdMNBus"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ilOYNQQyYHM",
        "outputId": "a9f65440-a43e-46ee-e35d-386f3c179056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.\n",
            "-----\n",
            "datasets            1.18.3\n",
            "datasets_modules    NA\n",
            "google              NA\n",
            "numpy               1.21.5\n",
            "pandas              1.3.5\n",
            "prettytable         3.1.1\n",
            "sinfo               0.3.4\n",
            "sklearn             1.0.2\n",
            "torch               1.10.0+cu111\n",
            "transformers        4.16.2\n",
            "-----\n",
            "IPython             5.5.0\n",
            "jupyter_client      5.3.5\n",
            "jupyter_core        4.9.2\n",
            "notebook            5.3.1\n",
            "-----\n",
            "Python 3.7.12 (default, Jan 15 2022, 18:48:18) [GCC 7.5.0]\n",
            "Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2 logical CPU cores, x86_64\n",
            "-----\n",
            "Session information updated at 2022-02-24 16:06\n"
          ]
        }
      ],
      "source": [
        "#############################\n",
        "## Print system info\n",
        "#############################\n",
        "#!pip install sinfo\n",
        "#import sinfo from sinfo\n",
        "sinfo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBvoHHMf_jc3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ek9ZdP0_jl6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ny-W5NU_jtR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Transformers_NER_FineTune_i2b2_2014_DEID_Roberta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c58982d26ddf4df4828f0b1a23e24aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5ef58f824d0e439dae33469d75ceeeca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d5a89819db944ab38db2b9aa743c55de",
              "IPY_MODEL_fb9519b26f604793b54fedf5f690402e",
              "IPY_MODEL_bed41815973640a7bc24e65103a402ef"
            ]
          }
        },
        "5ef58f824d0e439dae33469d75ceeeca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5a89819db944ab38db2b9aa743c55de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed2e3c538384418eacd8c3c4db8ec63b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17fda8b469db40a4895b9676d5d01c5c"
          }
        },
        "fb9519b26f604793b54fedf5f690402e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a88b7f28161040f1a4daf9b612ea1bba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2472,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2472,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_621cbc4f3cd24248b9d518a62eeac120"
          }
        },
        "bed41815973640a7bc24e65103a402ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0eb53e2443ae48d39b8d844a0fb2fea1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6.33k/? [00:00&lt;00:00, 96.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7318fdaf0074f8e9592c2bcdf171f17"
          }
        },
        "ed2e3c538384418eacd8c3c4db8ec63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17fda8b469db40a4895b9676d5d01c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a88b7f28161040f1a4daf9b612ea1bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "621cbc4f3cd24248b9d518a62eeac120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0eb53e2443ae48d39b8d844a0fb2fea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7318fdaf0074f8e9592c2bcdf171f17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}